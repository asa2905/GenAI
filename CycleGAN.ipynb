{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport csv\nimport shutil\n\ndef load_celeba_dataset(dataset_dir, output_dir, max_images=500):\n    dataset = {'men_without_glasses': [], 'men_with_glasses': [], 'women_with_glasses': []}\n    counts = {'men_without_glasses': 0, 'men_with_glasses': 0, 'women_with_glasses': 0}\n\n    # Read attributes from CSV file\n    attr_file_path = os.path.join(dataset_dir, 'list_attr_celeba.csv')\n    with open(attr_file_path, 'r') as f:\n        reader = csv.reader(f)\n        attributes = next(reader)[1:]  # Skip first column (filename)\n\n        for row in reader:\n            if counts['men_without_glasses'] >= max_images and counts['men_with_glasses'] >= max_images and counts['women_with_glasses'] >= max_images:\n                break  # Stop loading more images once we reach the limit for all categories\n\n            filename = row[0]\n            attr_values = row[1:]\n            img_path = os.path.join(dataset_dir, 'img_align_celeba/img_align_celeba', filename)\n\n            # Determine categories based on attributes\n            if int(attr_values[attributes.index('Male')]) == 1:\n                if int(attr_values[attributes.index('Eyeglasses')]) == 1 and counts['men_with_glasses'] < max_images:\n                    dataset['men_with_glasses'].append(img_path)\n                    counts['men_with_glasses'] += 1\n                elif int(attr_values[attributes.index('Eyeglasses')]) == -1 and counts['men_without_glasses'] < max_images:\n                    dataset['men_without_glasses'].append(img_path)\n                    counts['men_without_glasses'] += 1\n            else:\n                if int(attr_values[attributes.index('Eyeglasses')]) == 1 and counts['women_with_glasses'] < max_images:\n                    dataset['women_with_glasses'].append(img_path)\n                    counts['women_with_glasses'] += 1\n\n    # Split images into train and test sets\n    split_ratio = 0.8  # Adjust this ratio as needed\n    train = {'men_with_glasses': [], 'men_without_glasses': [], 'women_with_glasses': []}\n    test = {'men_with_glasses': [], 'men_without_glasses': [], 'women_with_glasses': []}\n\n    for category in ['men_with_glasses', 'men_without_glasses', 'women_with_glasses']:\n        train_size = int(len(dataset[category]) * split_ratio)\n        train[category] = dataset[category][:train_size]\n        test[category] = dataset[category][train_size:]\n\n    # Move images to respective directories\n    for category in ['men_with_glasses', 'men_without_glasses', 'women_with_glasses']:\n        for img_path in train[category]:\n            shutil.copy(img_path, os.path.join(output_dir, category, 'train', os.path.basename(img_path)))\n        for img_path in test[category]:\n            shutil.copy(img_path, os.path.join(output_dir, category, 'test', os.path.basename(img_path)))\n\n    return dataset\n\n# Example usage\ndataset_dir = '/kaggle/input/celeba-dataset/'\noutput_dir = '/kaggle/working/'  # Adjust this to your desired output directory\nos.makedirs(os.path.join(output_dir, 'men_with_glasses', 'train'), exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'men_with_glasses', 'test'), exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'men_without_glasses', 'train'), exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'men_without_glasses', 'test'), exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'women_with_glasses', 'train'), exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'women_with_glasses', 'test'), exist_ok=True)\nceleba_dataset = load_celeba_dataset(dataset_dir, output_dir, max_images=500)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T13:23:10.402853Z","iopub.execute_input":"2024-04-30T13:23:10.403542Z","iopub.status.idle":"2024-04-30T13:23:12.777545Z","shell.execute_reply.started":"2024-04-30T13:23:10.403507Z","shell.execute_reply":"2024-04-30T13:23:12.776649Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport numpy as np\nimport scipy.misc as misc\nfrom PIL import Image\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:57:50.549830Z","iopub.execute_input":"2024-04-30T11:57:50.550171Z","iopub.status.idle":"2024-04-30T11:58:08.767142Z","shell.execute_reply.started":"2024-04-30T11:57:50.550143Z","shell.execute_reply":"2024-04-30T11:58:08.766145Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-30 11:57:53.826115: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 11:57:53.826210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 11:57:54.070795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"epsilon = 1e-8\nimg_W = 128\nimg_H = 128\nimg_nums = 8000\nbatchsize = 5\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:58:31.264561Z","iopub.execute_input":"2024-04-30T11:58:31.265423Z","iopub.status.idle":"2024-04-30T11:58:31.269679Z","shell.execute_reply.started":"2024-04-30T11:58:31.265391Z","shell.execute_reply":"2024-04-30T11:58:31.268744Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Convolutional Layer\ndef conv(inputs, nums_out, ksize, stride, padding, is_dis=False):\n    c = int(inputs.shape[-1])\n    W = tf.get_variable(\"W\", shape=[ksize, ksize, c, nums_out], initializer=tf.random_normal_initializer(stddev=0.02))\n    b = tf.get_variable(\"b\", shape=[nums_out], initializer=tf.constant_initializer([0]))\n    if is_dis:\n        return tf.nn.conv2d(inputs, spectral_norm(\"SN\",W), [1, stride, stride, 1], padding) + b\n    else:\n        return tf.nn.conv2d(inputs, W, [1, stride, stride, 1], padding) + b\n# Transposed Convolutional Layer\ndef deconv(inputs, nums_out, ksize, stride):\n    c = int(inputs.shape[-1])\n    batch = int(inputs.shape[0])\n    height = int(inputs.shape[1])\n    width = int(inputs.shape[2])\n    W = tf.get_variable(\"W\", shape=[ksize, ksize, nums_out, c], initializer=tf.random_normal_initializer(stddev=0.02))\n    b = tf.get_variable(\"b\", shape=[nums_out], initializer=tf.constant_initializer([0.]))\n    return tf.nn.conv2d_transpose(inputs, W, output_shape=[batch, height*stride, width*stride, nums_out], strides=[1, stride, stride, 1]) + b\n\n# Instance Normalization\n# def InstanceNorm(inputs):\n#     mean, var = tf.nn.moments(inputs, axes=[1, 2], keep_dims=True)\n#     scale = tf.get_variable(\"scale\", shape=mean.shape[-1], initializer=tf.constant_initializer([1.]))\n#     shift = tf.get_variable(\"shift\", shape=mean.shape[-1], initializer=tf.constant_initializer([0.]))\n#     return (inputs - mean) * scale / tf.sqrt(var + epsilon) + shift\ndef InstanceNorm(inputs, name=None):\n    with tf.compat.v1.variable_scope(name, default_name='InstanceNorm'):\n        mean, var = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n        scale = tf.compat.v1.get_variable(\"scale\", shape=mean.shape[-1], initializer=tf.constant_initializer([1.]), trainable=True)\n        shift = tf.compat.v1.get_variable(\"shift\", shape=mean.shape[-1], initializer=tf.constant_initializer([0.]), trainable=True)\n        epsilon = 1e-8\n        return (inputs - mean) * scale / tf.sqrt(var + epsilon) + shift\n\n\n# Leaky ReLU activation\ndef leaky_relu(inputs, slope=0.2):\n    return tf.maximum(slope*inputs, inputs)\n\n# Spectral Normalization\ndef spectral_norm(name, w, iteration=1):\n    # Implementation of Spectral Normalization\n    # Code adapted from: https://github.com/taki0112/Spectral_Normalization-Tensorflow\n    w_shape = w.shape.as_list()\n    w = tf.reshape(w, [-1, w_shape[-1]])\n    with tf.variable_scope(name, reuse=False):\n        u = tf.get_variable(\"u\", [1, w_shape[-1]], initializer=tf.truncated_normal_initializer(), trainable=False)\n    u_hat = u\n    v_hat = None\n\n    def l2_norm(v, eps=1e-12):\n        return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n\n    for i in range(iteration):\n        v_ = tf.matmul(u_hat, tf.transpose(w))\n        v_hat = l2_norm(v_)\n        u_ = tf.matmul(v_hat, w)\n        u_hat = l2_norm(u_)\n    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n    w_norm = w / sigma\n    with tf.control_dependencies([u.assign(u_hat)]):\n        w_norm = tf.reshape(w_norm, w_shape)\n    return w_norm","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:58:32.579913Z","iopub.execute_input":"2024-04-30T11:58:32.580600Z","iopub.status.idle":"2024-04-30T11:58:32.600465Z","shell.execute_reply.started":"2024-04-30T11:58:32.580567Z","shell.execute_reply":"2024-04-30T11:58:32.599298Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# # Discriminator Class\n# class discriminator:\n#     def __init__(self, name):\n#         self.name = name\n\n#     def __call__(self, inputs, reuse=False):\n#         # Patch discriminator\n#         inputs = tf.random_crop(inputs, [batchsize, 70, 70, 3])\n#         with tf.variable_scope(self.name, reuse=reuse):\n#             with tf.variable_scope(\"c64\"):\n#                 inputs = leaky_relu(conv(inputs, 64, 5, 2, \"SAME\", True))\n#             with tf.variable_scope(\"c128\"):\n#                 inputs = leaky_relu(InstanceNorm(conv(inputs, 128, 5, 2, \"SAME\", True)))\n#             with tf.variable_scope(\"c256\"):\n#                 inputs = leaky_relu(InstanceNorm(conv(inputs, 256, 5, 2, \"SAME\", True)))\n#             with tf.variable_scope(\"c512\"):\n#                 inputs = leaky_relu(InstanceNorm(conv(inputs, 512, 5, 2, \"SAME\", True)))\n#             with tf.variable_scope(\"fully_conv\"):\n#                 ksize = np.size(inputs, 1)\n#                 inputs = tf.squeeze(conv(inputs, 1, ksize, 1, \"VALID\", True), axis=[1, 2, 3])\n#         return inputs\n\n#     @property\n#     def var(self):\n#         return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\nclass discriminator:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, inputs, reuse=tf.AUTO_REUSE):\n        with tf.compat.v1.variable_scope(self.name, reuse=reuse):\n            # Patch discriminator\n            inputs = tf.image.random_crop(inputs, [batchsize, 70, 70, 3])\n            with tf.compat.v1.variable_scope(\"c64\"):\n                inputs = leaky_relu(conv(inputs, 64, 5, 2, \"SAME\", True))\n            with tf.compat.v1.variable_scope(\"c128\"):\n                inputs = leaky_relu(InstanceNorm(conv(inputs, 128, 5, 2, \"SAME\", True)))\n            with tf.compat.v1.variable_scope(\"c256\"):\n                inputs = leaky_relu(InstanceNorm(conv(inputs, 256, 5, 2, \"SAME\", True)))\n            with tf.compat.v1.variable_scope(\"c512\"):\n                inputs = leaky_relu(InstanceNorm(conv(inputs, 512, 5, 2, \"SAME\", True)))\n            with tf.compat.v1.variable_scope(\"fully_conv\"):\n                ksize = np.size(inputs, 1)\n                inputs = tf.squeeze(conv(inputs, 1, ksize, 1, \"VALID\", True), axis=[1, 2, 3])\n        return inputs\n\n    @property\n    def var(self):\n        return tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:58:33.535200Z","iopub.execute_input":"2024-04-30T11:58:33.535601Z","iopub.status.idle":"2024-04-30T11:58:33.548133Z","shell.execute_reply.started":"2024-04-30T11:58:33.535572Z","shell.execute_reply":"2024-04-30T11:58:33.547085Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# # Generator Class\n# class generator:\n#     def __init__(self, name):\n#         self.name = name\n\n#     def __call__(self, inputs, reuse=False):\n#         with tf.variable_scope(name_or_scope=self.name, reuse=reuse):\n#             inputs = tf.pad(inputs, tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]]))\n#             with tf.variable_scope(\"c7s1-32\"):\n#                 inputs = tf.nn.relu(InstanceNorm(conv(inputs, 32, 7, 1, \"VALID\")))\n#             with tf.variable_scope(\"d64\"):\n#                 inputs = tf.nn.relu(InstanceNorm(conv(inputs, 64, 3, 2, \"SAME\")))\n#             with tf.variable_scope(\"d128\"):\n#                 inputs = tf.nn.relu(InstanceNorm(conv(inputs, 128, 3, 2, \"SAME\")))\n#             for i in range(6):\n#                 with tf.variable_scope(\"R\"+str(i)):\n#                     temp = inputs\n#                     with tf.variable_scope(\"R_conv1\"):\n#                         inputs = tf.pad(inputs, tf.constant([[0, 0], [1, 1], [1, 1], [0, 0]]), \"REFLECT\")\n#                         inputs = tf.nn.relu(InstanceNorm(conv(inputs, 128, 3, 1, \"VALID\")))\n#                     with tf.variable_scope(\"R_conv2\"):\n#                         inputs = tf.pad(inputs, tf.constant([[0, 0], [1, 1], [1, 1], [0, 0]]), \"REFLECT\")\n#                         inputs = InstanceNorm(conv(inputs, 128, 3, 1, \"VALID\"))\n#                     inputs = temp + inputs\n#             with tf.variable_scope(\"u64\"):\n#                 inputs = tf.nn.relu(InstanceNorm(deconv(inputs, 64, 3, 2)))\n#             with tf.variable_scope(\"u32\"):\n#                 inputs = tf.nn.relu(InstanceNorm(deconv(inputs, 32, 3, 2)))\n#             with tf.variable_scope(\"c7s1-3\"):\n#                 inputs = tf.nn.tanh((deconv(inputs, 3, 7, 1)))\n#             return (inputs + 1.) * 127.5\n\n#     @property\n#     def var(self):\n#         return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\nclass generator:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, inputs, reuse=tf.AUTO_REUSE):\n        with tf.compat.v1.variable_scope(name_or_scope=self.name, reuse=reuse):\n            inputs = tf.pad(inputs, tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]]))\n            with tf.compat.v1.variable_scope(\"c7s1-32\"):\n                inputs = tf.nn.relu(InstanceNorm(conv(inputs, 32, 7, 1, \"VALID\")))\n                inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n            with tf.compat.v1.variable_scope(\"d64\"):\n                inputs = tf.nn.relu(InstanceNorm(conv(inputs, 64, 3, 2, \"SAME\")))\n                inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n            with tf.compat.v1.variable_scope(\"d128\"):\n                inputs = tf.nn.relu(InstanceNorm(conv(inputs, 128, 3, 2, \"SAME\")))\n                inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n            for i in range(6):\n                with tf.compat.v1.variable_scope(\"R\" + str(i)):\n                    temp = inputs\n                    with tf.compat.v1.variable_scope(\"R_conv1\"):\n                        inputs = tf.pad(inputs, tf.constant([[0, 0], [1, 1], [1, 1], [0, 0]]), \"REFLECT\")\n                        inputs = tf.nn.relu(InstanceNorm(conv(inputs, 128, 3, 1, \"VALID\")))\n                        inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n                    with tf.compat.v1.variable_scope(\"R_conv2\"):\n                        inputs = tf.pad(inputs, tf.constant([[0, 0], [1, 1], [1, 1], [0, 0]]), \"REFLECT\")\n                        inputs = InstanceNorm(conv(inputs, 128, 3, 1, \"VALID\"))\n                        inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n                    inputs = temp + inputs\n            with tf.compat.v1.variable_scope(\"u64\"):\n                inputs = tf.nn.relu(InstanceNorm(deconv(inputs, 64, 3, 2)))\n                inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n            with tf.compat.v1.variable_scope(\"u32\"):\n                inputs = tf.nn.relu(InstanceNorm(deconv(inputs, 32, 3, 2)))\n                inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n            with tf.compat.v1.variable_scope(\"c7s1-3\"):\n                inputs = tf.nn.tanh((deconv(inputs, 3, 7, 1)))\n                inputs = tf.image.resize(inputs, [128, 128])  # Resize to match the input shape\n            return (inputs + 1.) * 127.5\n\n    @property\n    def var(self):\n        return tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:58:34.494768Z","iopub.execute_input":"2024-04-30T11:58:34.495351Z","iopub.status.idle":"2024-04-30T11:58:34.513345Z","shell.execute_reply.started":"2024-04-30T11:58:34.495320Z","shell.execute_reply":"2024-04-30T11:58:34.512473Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def resize_image(img_path):\n    img = Image.open(img_path)\n    img = img.resize((img_W, img_H), Image.BILINEAR)\n    return np.array(img)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:58:35.389368Z","iopub.execute_input":"2024-04-30T11:58:35.390138Z","iopub.status.idle":"2024-04-30T11:58:35.394715Z","shell.execute_reply.started":"2024-04-30T11:58:35.390109Z","shell.execute_reply":"2024-04-30T11:58:35.393785Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"output_dir = '/kaggle/working/'\nos.makedirs(os.path.join(output_dir, 'fake_Y'), exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'fake_X'), exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T12:43:49.919970Z","iopub.execute_input":"2024-04-30T12:43:49.920643Z","iopub.status.idle":"2024-04-30T12:43:49.925884Z","shell.execute_reply.started":"2024-04-30T12:43:49.920609Z","shell.execute_reply":"2024-04-30T12:43:49.925002Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def count_images_in_directory(directory):\n    count = 0\n    for file in os.listdir(directory):\n        if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.jpeg'):\n            count += 1\n    return count\n\n# Example usage\ndirectory_path = '/kaggle/working/women_with_glasses/train'\nimage_count = count_images_in_directory(directory_path)\nprint(\"Number of images:\", image_count)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T13:51:36.465879Z","iopub.execute_input":"2024-04-30T13:51:36.466301Z","iopub.status.idle":"2024-04-30T13:51:36.474173Z","shell.execute_reply.started":"2024-04-30T13:51:36.466270Z","shell.execute_reply":"2024-04-30T13:51:36.473216Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Number of images: 400\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-30T12:31:15.226034Z","iopub.execute_input":"2024-04-30T12:31:15.226436Z","iopub.status.idle":"2024-04-30T12:31:15.352199Z","shell.execute_reply.started":"2024-04-30T12:31:15.226404Z","shell.execute_reply":"2024-04-30T12:31:15.351247Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# CycleGAN Class\nclass CycleGAN:\n    def __init__(self):\n        self.X = tf.placeholder(\"float\", shape=[batchsize, img_H, img_W, 3])\n        self.Y = tf.placeholder(\"float\", shape=[batchsize, img_H, img_W, 3])\n        G = generator(\"G\")\n        F = generator(\"F\")\n        self.Dx = discriminator(\"Dx\")\n        self.Dy = discriminator(\"Dy\")\n        self.fake_X = F(self.Y)\n        self.fake_Y = G(self.X)\n        self.logits_real_X = self.Dx(self.X)\n        self.logits_real_Y = self.Dy(self.Y)\n        self.logits_fake_X = self.Dx(self.fake_X, True)\n        self.logits_fake_Y = self.Dy(self.fake_Y, True)\n        self.L_cyc = tf.reduce_mean(tf.abs(F(self.fake_Y, True) - self.X)) + tf.reduce_mean(tf.abs(G(self.fake_X, True) - self.Y))\n        self.d_loss_Y = -tf.reduce_mean(self.logits_real_Y) + tf.reduce_mean(self.logits_fake_Y)\n        self.d_loss_X = -tf.reduce_mean(self.logits_real_X) + tf.reduce_mean(self.logits_fake_X)\n        self.g_loss_Y = -tf.reduce_mean(self.logits_fake_Y) + 10. * self.L_cyc\n        self.g_loss_X = -tf.reduce_mean(self.logits_fake_X) + 10. * self.L_cyc\n#         self.Dx_opt = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.d_loss_X, var_list=[self.Dx.var])\n#         self.Dy_opt = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.d_loss_Y, var_list=[self.Dy.var])\n#         self.G_opt = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.g_loss_Y, var_list=G.var)\n#         self.F_opt = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.g_loss_X, var_list=F.var)\n        with tf.compat.v1.variable_scope(\"optimizer\", reuse=tf.compat.v1.AUTO_REUSE):\n            self.Dx_opt = tf.compat.v1.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.d_loss_X, var_list=[self.Dx.var])\n            self.Dy_opt = tf.compat.v1.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.d_loss_Y, var_list=[self.Dy.var])\n            self.G_opt = tf.compat.v1.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.g_loss_Y, var_list=G.var)\n            self.F_opt = tf.compat.v1.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.g_loss_X, var_list=F.var)\n\n\n\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        self.train()\n\n    def train(self):\n        Y_path = \"/kaggle/working/women_with_glasses/train/\"\n        X_path = \"/kaggle/working/men_with_glasses/train/\"\n        Y = os.listdir(Y_path)[:img_nums]\n        X = os.listdir(X_path)[:img_nums]\n        nums = len(Y)\n        saver = tf.train.Saver()\n        for epoch in range(10):\n            print(epoch);\n            for i in range(int(nums / batchsize) - 1):\n                X_img = np.zeros([batchsize, img_H, img_W, 3])\n                Y_img = np.zeros([batchsize, img_H, img_W, 3])\n                for j in np.arange(i * batchsize, i * batchsize + batchsize, 1):\n                    img = resize_image(X_path + X[j])\n                    X_img[j - i * batchsize, :, :, :] = img\n                    img = resize_image(Y_path + Y[j])\n                    Y_img[j - i * batchsize, :, :, :] = img\n                self.sess.run(self.Dy_opt, feed_dict={self.X: X_img, self.Y: Y_img})\n                self.sess.run(self.Dx_opt, feed_dict={self.X: X_img, self.Y: Y_img})\n                self.sess.run(self.G_opt, feed_dict={self.X: X_img, self.Y: Y_img})\n                self.sess.run(self.F_opt, feed_dict={self.X: X_img, self.Y: Y_img})\n                if i % 10 == 0:\n                    [d_loss_X, d_loss_Y, g_loss_Y, g_loss_X, fake_X, fake_Y, cyc_loss] = \\\n                        self.sess.run([self.d_loss_X, self.d_loss_Y, self.g_loss_Y, self.g_loss_X, self.fake_X, self.fake_Y, self.L_cyc], feed_dict={self.X: X_img, self.Y: Y_img})\n                    print(\"epoch: %d, step: %d, d_loss_X: %g, d_loss_Y: %g, g_loss_X: %g, g_loss_Y: %g, cyc_loss: %g\"%(epoch, i, d_loss_X, d_loss_Y, g_loss_X, g_loss_Y, cyc_loss))\n                    Image.fromarray(np.uint8(fake_Y)[0, :, :, :]).save(\"/kaggle/working/fake_Y/\"+str(epoch)+\"_\"+str(i)+\".jpg\")\n                    Image.fromarray(np.uint8(fake_X)[0, :, :, :]).save(\"/kaggle/working/fake_X/\" + str(epoch) + \"_\" + str(i) + \".jpg\")\n            saver.save(self.sess, \"./save_para//CycleGAN_man_woman.ckpt\")\n\\\nif __name__ == \"__main__\":\n    cyc = CycleGAN()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T13:52:29.765879Z","iopub.execute_input":"2024-04-30T13:52:29.766273Z","iopub.status.idle":"2024-04-30T14:31:48.754896Z","shell.execute_reply.started":"2024-04-30T13:52:29.766220Z","shell.execute_reply":"2024-04-30T14:31:48.753825Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"0\nepoch: 0, step: 0, d_loss_X: -0.662287, d_loss_Y: -0.585405, g_loss_X: 1266.29, g_loss_Y: 1264.96, cyc_loss: 126.562\nepoch: 0, step: 10, d_loss_X: 1.36976, d_loss_Y: 2.26612, g_loss_X: 612.414, g_loss_Y: 613.968, cyc_loss: 61.7772\nepoch: 0, step: 20, d_loss_X: -7.28175, d_loss_Y: -4.82588, g_loss_X: 778.058, g_loss_Y: 775.683, cyc_loss: 78.0544\nepoch: 0, step: 30, d_loss_X: -5.50047, d_loss_Y: -2.75253, g_loss_X: 631.894, g_loss_Y: 636.349, cyc_loss: 64.0374\nepoch: 0, step: 40, d_loss_X: 11.7083, d_loss_Y: 0.358695, g_loss_X: 694.663, g_loss_Y: 721.594, cyc_loss: 72.3764\nepoch: 0, step: 50, d_loss_X: -3.22335, d_loss_Y: -22.5157, g_loss_X: 1102.53, g_loss_Y: 1124.25, cyc_loss: 111.807\nepoch: 0, step: 60, d_loss_X: -7.94127, d_loss_Y: -5.23399, g_loss_X: 1519.51, g_loss_Y: 1502.39, cyc_loss: 151.623\nepoch: 0, step: 70, d_loss_X: -29.6034, d_loss_Y: -0.147729, g_loss_X: 756.034, g_loss_Y: 741.365, cyc_loss: 74.7483\n1\nepoch: 1, step: 0, d_loss_X: -2.2395, d_loss_Y: -6.08209, g_loss_X: 611.949, g_loss_Y: 624.607, cyc_loss: 61.5906\nepoch: 1, step: 10, d_loss_X: 2.65989, d_loss_Y: 10.1758, g_loss_X: 587.245, g_loss_Y: 585.349, cyc_loss: 59.8246\nepoch: 1, step: 20, d_loss_X: -1.27843, d_loss_Y: 7.66918, g_loss_X: 526.919, g_loss_Y: 494.475, cyc_loss: 52.1182\nepoch: 1, step: 30, d_loss_X: 1.04392, d_loss_Y: -0.124393, g_loss_X: 616.849, g_loss_Y: 617.6, cyc_loss: 62.5707\nepoch: 1, step: 40, d_loss_X: -1.40598, d_loss_Y: -1.27934, g_loss_X: 734.751, g_loss_Y: 761.044, cyc_loss: 75.2094\nepoch: 1, step: 50, d_loss_X: 11.4025, d_loss_Y: 9.92647, g_loss_X: 499.459, g_loss_Y: 490.009, cyc_loss: 50.3173\nepoch: 1, step: 60, d_loss_X: 20.4738, d_loss_Y: -46.8153, g_loss_X: 459.626, g_loss_Y: 493.53, cyc_loss: 47.5808\nepoch: 1, step: 70, d_loss_X: -13.4486, d_loss_Y: 17.7138, g_loss_X: 524.678, g_loss_Y: 504.771, cyc_loss: 53.7766\n2\nepoch: 2, step: 0, d_loss_X: 18.8729, d_loss_Y: -25.1149, g_loss_X: 455.887, g_loss_Y: 471.693, cyc_loss: 47.8463\nepoch: 2, step: 10, d_loss_X: -15.9128, d_loss_Y: 7.51431, g_loss_X: 442.017, g_loss_Y: 432.617, cyc_loss: 43.9601\nepoch: 2, step: 20, d_loss_X: -13.6254, d_loss_Y: -1.33543, g_loss_X: 495.472, g_loss_Y: 476.686, cyc_loss: 50.0342\nepoch: 2, step: 30, d_loss_X: 2.49447, d_loss_Y: -20.1863, g_loss_X: 509.504, g_loss_Y: 500.267, cyc_loss: 51.4644\nepoch: 2, step: 40, d_loss_X: -4.68716, d_loss_Y: -26.3493, g_loss_X: 557.912, g_loss_Y: 577.325, cyc_loss: 56.2804\nepoch: 2, step: 50, d_loss_X: -31.4833, d_loss_Y: -5.63625, g_loss_X: 443.238, g_loss_Y: 412.761, cyc_loss: 43.1992\nepoch: 2, step: 60, d_loss_X: -0.775605, d_loss_Y: -42.3324, g_loss_X: 443.566, g_loss_Y: 480.481, cyc_loss: 44.5257\nepoch: 2, step: 70, d_loss_X: -34.9215, d_loss_Y: 6.51718, g_loss_X: 358.961, g_loss_Y: 311.536, cyc_loss: 34.9563\n3\nepoch: 3, step: 0, d_loss_X: -16.1147, d_loss_Y: -36.1069, g_loss_X: 541.245, g_loss_Y: 581.669, cyc_loss: 55.3343\nepoch: 3, step: 10, d_loss_X: -27.4849, d_loss_Y: 16.1857, g_loss_X: 489.326, g_loss_Y: 455.092, cyc_loss: 49.2535\nepoch: 3, step: 20, d_loss_X: -5.45213, d_loss_Y: 18.7891, g_loss_X: 440.153, g_loss_Y: 417.588, cyc_loss: 43.1347\nepoch: 3, step: 30, d_loss_X: -14.3695, d_loss_Y: -15.3326, g_loss_X: 503.044, g_loss_Y: 477.872, cyc_loss: 50.5992\nepoch: 3, step: 40, d_loss_X: -27.7992, d_loss_Y: 41.0172, g_loss_X: 607.977, g_loss_Y: 546.014, cyc_loss: 58.9165\nepoch: 3, step: 50, d_loss_X: -31.1887, d_loss_Y: -49.6992, g_loss_X: 398.561, g_loss_Y: 432.014, cyc_loss: 39.4392\nepoch: 3, step: 60, d_loss_X: -35.0193, d_loss_Y: -38.6337, g_loss_X: 429.731, g_loss_Y: 439.364, cyc_loss: 41.4586\nepoch: 4, step: 50, d_loss_X: -60.0084, d_loss_Y: 0.85714, g_loss_X: 511.43, g_loss_Y: 528.677, cyc_loss: 48.9175\nepoch: 4, step: 60, d_loss_X: -34.6303, d_loss_Y: -79.7483, g_loss_X: 438.577, g_loss_Y: 492.866, cyc_loss: 44.2372\nepoch: 4, step: 70, d_loss_X: 14.3209, d_loss_Y: -58.2218, g_loss_X: 292.893, g_loss_Y: 374.498, cyc_loss: 32.5135\n5\nepoch: 5, step: 0, d_loss_X: -11.0531, d_loss_Y: -19.0713, g_loss_X: 409.758, g_loss_Y: 369.649, cyc_loss: 37.5327\nepoch: 5, step: 10, d_loss_X: -4.98125, d_loss_Y: 24.4165, g_loss_X: 466.023, g_loss_Y: 420.212, cyc_loss: 45.1329\nepoch: 5, step: 20, d_loss_X: 6.42228, d_loss_Y: -33.4158, g_loss_X: 574.088, g_loss_Y: 535.947, cyc_loss: 54.9844\nepoch: 5, step: 30, d_loss_X: -23.7937, d_loss_Y: -37.9211, g_loss_X: 592.818, g_loss_Y: 617.731, cyc_loss: 60.8862\nepoch: 5, step: 40, d_loss_X: -30.4621, d_loss_Y: 27.3461, g_loss_X: 1142.71, g_loss_Y: 1146.4, cyc_loss: 115.83\nepoch: 5, step: 50, d_loss_X: -33.1473, d_loss_Y: -14.6787, g_loss_X: 443.881, g_loss_Y: 423.733, cyc_loss: 44.1952\nepoch: 5, step: 60, d_loss_X: -15.191, d_loss_Y: -50.9133, g_loss_X: 388.541, g_loss_Y: 474.955, cyc_loss: 42.8142\nepoch: 5, step: 70, d_loss_X: 9.30643, d_loss_Y: 5.52617, g_loss_X: 346.435, g_loss_Y: 319.37, cyc_loss: 34.7457\n6\nepoch: 6, step: 0, d_loss_X: -28.9241, d_loss_Y: 10.737, g_loss_X: 518.274, g_loss_Y: 415.155, cyc_loss: 46.4139\nepoch: 6, step: 10, d_loss_X: -27.8612, d_loss_Y: -38.8853, g_loss_X: 411.567, g_loss_Y: 409.134, cyc_loss: 40.2363\nepoch: 6, step: 20, d_loss_X: -46.3838, d_loss_Y: -35.678, g_loss_X: 642.207, g_loss_Y: 581.944, cyc_loss: 58.9479\nepoch: 6, step: 30, d_loss_X: -10.9754, d_loss_Y: -3.2007, g_loss_X: 440.302, g_loss_Y: 475.528, cyc_loss: 48.511\nepoch: 6, step: 40, d_loss_X: -32.4663, d_loss_Y: -51.4654, g_loss_X: 697.692, g_loss_Y: 750.336, cyc_loss: 70.6317\nepoch: 6, step: 50, d_loss_X: -21.5722, d_loss_Y: 12.4795, g_loss_X: 345.677, g_loss_Y: 305.741, cyc_loss: 32.5112\nepoch: 6, step: 60, d_loss_X: 34.8618, d_loss_Y: -41.2055, g_loss_X: 821.913, g_loss_Y: 858.034, cyc_loss: 87.4737\nepoch: 6, step: 70, d_loss_X: 16.8769, d_loss_Y: 40.877, g_loss_X: 341.995, g_loss_Y: 262.989, cyc_loss: 31.7464\n7\nepoch: 7, step: 0, d_loss_X: -33.0836, d_loss_Y: -25.7397, g_loss_X: 463.108, g_loss_Y: 506.704, cyc_loss: 46.7544\nepoch: 7, step: 10, d_loss_X: -22.3808, d_loss_Y: -8.38792, g_loss_X: 426.726, g_loss_Y: 458.782, cyc_loss: 42.4641\nepoch: 7, step: 20, d_loss_X: -55.6515, d_loss_Y: -47.3227, g_loss_X: 706.707, g_loss_Y: 682.199, cyc_loss: 69.3321\nepoch: 7, step: 30, d_loss_X: -12.7029, d_loss_Y: -85.2488, g_loss_X: 449.42, g_loss_Y: 472.41, cyc_loss: 43.8924\nepoch: 7, step: 40, d_loss_X: -55.1069, d_loss_Y: -30.2602, g_loss_X: 530.913, g_loss_Y: 544.933, cyc_loss: 52.6108\nepoch: 7, step: 50, d_loss_X: -45.0879, d_loss_Y: -47.6793, g_loss_X: 467.556, g_loss_Y: 421.638, cyc_loss: 41.6135\nepoch: 7, step: 60, d_loss_X: -19.5615, d_loss_Y: -70.7036, g_loss_X: 459.751, g_loss_Y: 499.456, cyc_loss: 44.3927\nepoch: 7, step: 70, d_loss_X: 7.07302, d_loss_Y: -54.7191, g_loss_X: 529.621, g_loss_Y: 505.934, cyc_loss: 49.0768\n8\nepoch: 8, step: 0, d_loss_X: -15.8502, d_loss_Y: -44.1753, g_loss_X: 388.42, g_loss_Y: 427.779, cyc_loss: 41.5361\nepoch: 8, step: 10, d_loss_X: -37.6541, d_loss_Y: -55.634, g_loss_X: 437.003, g_loss_Y: 422.53, cyc_loss: 39.3649\nepoch: 8, step: 20, d_loss_X: -22.2323, d_loss_Y: 23.579, g_loss_X: 489.989, g_loss_Y: 406.853, cyc_loss: 45.5732\nepoch: 8, step: 30, d_loss_X: -18.7308, d_loss_Y: -70.6429, g_loss_X: 449.894, g_loss_Y: 505.268, cyc_loss: 45.1187\nepoch: 8, step: 40, d_loss_X: -9.34619, d_loss_Y: 3.61166, g_loss_X: 616.068, g_loss_Y: 652.231, cyc_loss: 59.6378\nepoch: 8, step: 50, d_loss_X: 37.1309, d_loss_Y: 25.0504, g_loss_X: 299.178, g_loss_Y: 267.617, cyc_loss: 33.1308\nepoch: 8, step: 60, d_loss_X: -58.4824, d_loss_Y: -123.294, g_loss_X: 508.841, g_loss_Y: 567.885, cyc_loss: 50.0167\nepoch: 8, step: 70, d_loss_X: 5.89881, d_loss_Y: -6.70633, g_loss_X: 417.489, g_loss_Y: 440.065, cyc_loss: 37.9196\n9\nepoch: 9, step: 0, d_loss_X: -18.039, d_loss_Y: 1.6991, g_loss_X: 449.769, g_loss_Y: 377.737, cyc_loss: 41.3428\nepoch: 9, step: 10, d_loss_X: -35.8497, d_loss_Y: -39.8552, g_loss_X: 489.432, g_loss_Y: 470.804, cyc_loss: 49.1679\nepoch: 9, step: 20, d_loss_X: -61.8607, d_loss_Y: -53.6111, g_loss_X: 468.681, g_loss_Y: 517.154, cyc_loss: 45.3215\nepoch: 9, step: 30, d_loss_X: -6.25535, d_loss_Y: -64.2583, g_loss_X: 556.371, g_loss_Y: 562.837, cyc_loss: 56.74\nepoch: 9, step: 40, d_loss_X: -20.7641, d_loss_Y: -53.327, g_loss_X: 579.222, g_loss_Y: 547.118, cyc_loss: 55.7247\nepoch: 9, step: 50, d_loss_X: -45.7117, d_loss_Y: -69.4833, g_loss_X: 379.499, g_loss_Y: 364.798, cyc_loss: 35.5358\nepoch: 9, step: 60, d_loss_X: -68.0602, d_loss_Y: -95.7388, g_loss_X: 555.015, g_loss_Y: 598.336, cyc_loss: 53.7618\nepoch: 9, step: 70, d_loss_X: -78.1368, d_loss_Y: 14.6516, g_loss_X: 870.476, g_loss_Y: 791.954, cyc_loss: 83.7325\n","output_type":"stream"}]},{"cell_type":"code","source":"input_image_path = \"/kaggle/working/men_with_glasses/test/008007.jpg\"\n\n# Load the input image and resize it\ninput_image = resize_image(input_image_path)\n\n# Generate the image with glasses\ngenerated_image = cyc.sess.run(cyc.fake_X, feed_dict={cyc.Y: np.tile([input_image], (5, 1, 1, 1))})\n\n\n# Convert the generated image to uint8 and save it\ngenerated_image = np.uint8(generated_image[0])\noutput_image = Image.fromarray(generated_image)\noutput_image.save(\"/kaggle/working/generated_image_women_with_glasses.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T14:47:59.861520Z","iopub.execute_input":"2024-04-30T14:47:59.862148Z","iopub.status.idle":"2024-04-30T14:48:00.067154Z","shell.execute_reply.started":"2024-04-30T14:47:59.862120Z","shell.execute_reply":"2024-04-30T14:48:00.066439Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"input_image_path = \"/kaggle/working/men_with_glasses/test/001721.jpg\"\n\n# Load the input image and resize it\ninput_image = resize_image(input_image_path)\n\n# Generate the image with glasses\ngenerated_image = cyc.sess.run(cyc.fake_X, feed_dict={cyc.Y: np.tile([input_image], (5, 1, 1, 1))})\n\n\n# Convert the generated image to uint8 and save it\ngenerated_image = np.uint8(generated_image[0])\noutput_image = Image.fromarray(generated_image)\noutput_image.save(\"/kaggle/working/generated_image_without_glasses2.jpg\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_image_path = \"/kaggle/working/women_with_glasses/test/035030.jpg\"\n\n# Load the input image and resize it\ninput_image = resize_image(input_image_path)\n\n# Generate the image with glasses\ngenerated_image = cyc.sess.run(cyc.fake_Y, feed_dict={cyc.X: np.tile([input_image], (5, 1, 1, 1))})\n\n\n# Convert the generated image to uint8 and save it\ngenerated_image = np.uint8(generated_image[0])\noutput_image = Image.fromarray(generated_image)\noutput_image.save(\"/kaggle/working/generated_image_men_with_glasses.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T14:56:20.869942Z","iopub.execute_input":"2024-04-30T14:56:20.870350Z","iopub.status.idle":"2024-04-30T14:56:21.077605Z","shell.execute_reply.started":"2024-04-30T14:56:20.870319Z","shell.execute_reply":"2024-04-30T14:56:21.076798Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"input_image_path = \"/kaggle/working/women_with_glasses/train/007468.jpg\"\n\n# Load the input image and resize it\ninput_image = resize_image(input_image_path)\n\n# Generate the image with glasses\ngenerated_image = cyc.sess.run(cyc.fake_Y, feed_dict={cyc.X: np.tile([input_image], (5, 1, 1, 1))})\n\n\n# Convert the generated image to uint8 and save it\ngenerated_image = np.uint8(generated_image[0])\noutput_image = Image.fromarray(generated_image)\noutput_image.save(\"/kaggle/working/generated_image_with_glasses2.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T14:55:11.403053Z","iopub.execute_input":"2024-04-30T14:55:11.403932Z","iopub.status.idle":"2024-04-30T14:55:11.610460Z","shell.execute_reply.started":"2024-04-30T14:55:11.403900Z","shell.execute_reply":"2024-04-30T14:55:11.609685Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}